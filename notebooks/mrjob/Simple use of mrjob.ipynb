{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##mrjob##\n",
    "\n",
    "__mrjob__ is a software package developed by the restaurant recommendation company _Yelp_. \n",
    "It's goal is to simplify the deployment of map-reduce jobs based on streaming and python onto different \n",
    "frameworks such as Hadoop on a private cluster or hadoop on AWS (called EMR).\n",
    "\n",
    "* You can read more about mrjob here: https://pythonhosted.org/mrjob/index.html  \n",
    "* and you can clone it from github here: https://github.com/yelp/mrjob\n",
    "\n",
    "In this notebook we run a simple word-count example, add to it some logging commands, and look at two modes of running the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get enviroment variables set from utils/setup.sh\n",
    "home_dir = os.environ['HOME']\n",
    "root_dir = os.environ['BD_GitRoot']\n",
    "\n",
    "# Add utils to the python system path\n",
    "sys.path.append(root_dir + '/utils')\n",
    "\n",
    "# Read AWS credentials from 'EC2_VAULT'/Creds.pkl \n",
    "from read_mrjob_creds import *\n",
    "(key_id, secret_key, s3_bucket, username) = read_credentials()\n",
    "\n",
    "examples_dir = root_dir + '/notebooks/mrjob/'\n",
    "!ls -l $examples_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename=examples_dir+'mr_word_freq_count.py'\n",
    "print filename\n",
    "\n",
    "!ls -al $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load example code from mr jobs as a starting point\n",
    "%load $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mr_word_freq_count.py\n",
    "#!/usr/bin/python\n",
    "# Copyright 2009-2010 Yelp\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"The classic MapReduce job: count the frequency of words.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "from sys import stderr\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "#logfile=open('log','w')\n",
    "logfile=stderr\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            logfile.write('mapper '+word.lower()+'\\n')\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        #yield (word, sum(counts))\n",
    "        l_counts=[c for c in counts]  # extract list from iterator\n",
    "        S=sum(l_counts)\n",
    "        logfile.write('combiner '+word+' ['+','.join([str(c) for c in l_counts])+']='+str(S)+'\\n')\n",
    "        yield (word, S)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        #yield (word, sum(counts))\n",
    "        l_counts=[c for c in counts]  # extract list from iterator\n",
    "        S=sum(l_counts)\n",
    "        logfile.write('reducer '+word+' ['+','.join([str(c) for c in l_counts])+']='+str(S)+'\\n')\n",
    "        yield (word, S)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python mr_word_freq_count.py $root_dir/README.md > counts_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat counts_local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the meaning of \"yield\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword __yield__ is somewhat similar to __return__ however, while __return__ terminates the function and returns the result, \n",
    "__yield__, the first time it is encountered, return an object called a __generator__, without executing the function even once. On subsequent calls, the function is executed until one or more __yield__ commands are encountered, these values are returned, and the function halts (but does not terminate) until it is called again.\n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def myrange(start,stop,step):\n",
    "    value=start\n",
    "    while value<=stop:\n",
    "        yield value\n",
    "        value += step\n",
    "print [x for x in myrange(1.0,3.0,0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print myrange(1.0,3.0,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen1=myrange(1.0,3.0,0.3)\n",
    "gen2=myrange(2.0,5.0,0.7)\n",
    "print 'gen1:',[x for x in gen1]\n",
    "print 'gen1:',[x for x in gen1]  # after the generator terminated, it does not yield any more values.\n",
    "print 'gen2:',[x for x in gen2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator is similar to an array or a list, all of those are __iterable__ objects. However, while list store all of the values in memory and can be read in any order, generators create the values on the fly and can only traversed __once__ and __in order__\n",
    "\n",
    "It is the fact that values are generated on the fly and then discarded which makes generators attractive when processing large amounts of data - only a small amount of intermedite results, the outputs of the mapper which are inputs to the reducer, need to be stored in memory. How much depends on the communication speed between mappers and reducers.\n",
    "\n",
    "It is instructive to see how generators can be cascaded by passing a generator as a parameter to another generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mycumul(values):   # values can be a list or a generator.\n",
    "    s=0\n",
    "    for value in values:\n",
    "        s+=value\n",
    "        yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we pass a generator as an input to another generator.\n",
    "gen3=mycumul(myrange(1.0,3.0,0.3))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'gen3:',[x for x in gen3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different modes of running a mrjob map-reduce job ##\n",
    "\n",
    "Once the mapper, combiner and reducer have been written and tested, you can run the job on different types of infrastructure:\n",
    "\n",
    "1. __inline__ run the job as a single process on the local machine.\n",
    "1. __local__ run the job on the local machine, but using multiple processes to simulate parallel processing.\n",
    "1. __hadoop__ run the job on a hadoop cluster (such as the one we have in SDSC)\n",
    "1. __EMR__ (Elastic Map Reduce) run the job on a hadoop cluster running on the amazon cloud.\n",
    "\n",
    "Below we run the same process we ran at the top using __local__ instead of the default __inline__. Observe that in this case the reducers have some non-trivial work to do even when combiners are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python mr_word_freq_count.py --runner=local $root_dir/README.md > counts_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat counts_local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from find_waiting_flow import *\n",
    "flow_id = find_waiting_flow(key_id,secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running in EMR mode on existing job flow (hadoop cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create unique output directory in the student's s3_bucket\n",
    "output_dir = s3_bucket + str(uuid.uuid4()) + \"/\"\n",
    "\n",
    "print output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python mr_word_freq_count.py -r emr $root_dir/README.md --emr-job-flow-id=$flow_id --output-dir=$output_dir  > counts_emr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat counts_emr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: mr_travelling_salesman is missing from GitHub repo\n",
    "%load $root_dir/examples/mr_travelling_salesman/README.rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW ###\n",
    "1) Look around in the examples directory\n",
    "2) Write a map-reduce job that computes the PCA of a large set of vectors (use as input the max_temp profiles in \n",
    "   /home/ubuntu/data/weather/SAMPLE_TMAX.csv)\n",
    "\n",
    "**Hint:** One map-reduce job is enough. You might think that you first need to compute the means $\\mu_i=E(X_i)$ and then, in a second path, compute\n",
    "$$cov(X_i,X_j) = E((X_i-\\mu_i)(X_j-\\mu_j))$$\n",
    "However, recall the formula \n",
    "$$ var(X) \\doteq E((X-\\mu)^2) = E(X^2) - E(X)^2 $$\n",
    "This formula can be generalized to the $cov$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wc /home/ubuntu/data/weather/SAMPLE_TMAX.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
